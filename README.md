# KD-HGRL: Knowledge Distillation for Heterogeneous Graph Representation Learning

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.1.2](https://img.shields.io/badge/PyTorch-2.1.2-red.svg)](https://pytorch.org/)
[![CUDA 11.8](https://img.shields.io/badge/CUDA-11.8-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“– Overview

KD-HGRL is a comprehensive framework for **Knowledge Distillation in Heterogeneous Graph Representation Learning**. This project implements a **dual-teacher distillation architecture** that combines knowledge distillation with augmentation-based robustness learning to create compressed, efficient heterogeneous graph neural network models while maintaining competitive performance.

### ğŸ¯ Key Features

- **Dual-Teacher Architecture**: 
  - **Main Teacher**: Provides knowledge distillation from original graph data
  - **Augmentation Teacher**: Provides robustness guidance from augmented graph data
  - **Student**: Learns from both teachers with 50% parameter compression
- **Heterogeneous Graph Support**: ACM, DBLP, AMiner, Freebase datasets
- **Multi-View Learning**: Meta-path encoder + Schema-level encoder
- **Advanced Augmentation**: Structure-aware heterogeneous graph augmentation
- **Multi-Task Evaluation**: Node classification, link prediction, node clustering
- **Model Compression**: 50% parameter reduction with ~95% performance retention
- **GPU Acceleration**: CUDA 11.8 support with PyTorch 2.1.2
- **Modular Loss Components**: Configurable KD loss, augmentation alignment, link reconstruction

### ğŸ† Performance Highlights

| Model | Parameters | Compression | Node Classification | Link Prediction | Node Clustering |
|-------|------------|-------------|-------------------|-----------------|-----------------|
| Teacher | 100% | - | Baseline | Baseline | Baseline |
| Middle Teacher | 100% | No compression* | ~98% retention | ~97% retention | ~98% retention |
| Student | ~50% | 50% | ~95% retention | ~93% retention | ~94% retention |

\* *Middle teacher uses same architecture as teacher but trains on augmented data for robustness guidance*

## ï¿½ How It Works: Dual-Teacher Architecture

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DUAL-TEACHER FRAMEWORK                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  Original Graph â”‚         â”‚  Augmented Graph  â”‚              â”‚
â”‚  â”‚  - PAP, PSP     â”‚         â”‚  - Feature mask   â”‚              â”‚
â”‚  â”‚  - Clean data   â”‚         â”‚  - Edge perturb   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚           â”‚                           â”‚                         â”‚
â”‚           â–¼                           â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  Main Teacher   â”‚         â”‚ Augmentation     â”‚              â”‚
â”‚  â”‚  (100% params)  â”‚         â”‚ Teacher          â”‚              â”‚
â”‚  â”‚                 â”‚         â”‚ (100% params)    â”‚              â”‚
â”‚  â”‚  - Meta-path    â”‚         â”‚                  â”‚              â”‚
â”‚  â”‚  - Schema view  â”‚         â”‚ - Robust         â”‚              â”‚
â”‚  â”‚  - Contrastive  â”‚         â”‚   patterns       â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚           â”‚                           â”‚                         â”‚
â”‚           â”‚ KD Loss                   â”‚ Augmentation            â”‚
â”‚           â”‚ (knowledge)               â”‚ Alignment               â”‚
â”‚           â”‚                           â”‚ (robustness)            â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                     â”‚                                           â”‚
â”‚                     â–¼                                           â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚           â”‚  Student Model   â”‚                                  â”‚
â”‚           â”‚  (50% params)    â”‚                                  â”‚
â”‚           â”‚                  â”‚                                  â”‚
â”‚           â”‚  - Compressed    â”‚                                  â”‚
â”‚           â”‚  - Fast          â”‚                                  â”‚
â”‚           â”‚  - Robust        â”‚                                  â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                                                                  â”‚
â”‚  Loss = Student_Loss + Î±Â·KD_Loss + Î²Â·Aug_Align + Î³Â·Link_Loss   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Innovations

1. **Independent Teacher Training**: Both teachers train independently
   - Main teacher learns from clean data
   - Augmentation teacher learns from augmented data
   - No hierarchical dependency

2. **Dual-Source Knowledge Transfer**: Student learns from both
   - **Knowledge Distillation** (Main Teacher): Transferring learned representations
   - **Augmentation Alignment** (Aug Teacher): Learning robust patterns
   - **Self-Learning**: Student's own contrastive loss

3. **Multi-Loss Training**:
   ```python
   Total Loss = student_contrastive_loss 
              + main_distill_weight * kd_loss          # from main teacher
              + augmentation_weight * alignment_loss    # from aug teacher  
              + link_recon_weight * link_loss          # optional, edge modeling
   ```

4. **Heterogeneous Graph Augmentation**:
   - Feature masking (random node feature dropout)
   - Edge perturbation (meta-path sampling variations)
   - Structure-aware augmentation (preserving heterogeneity)

## ï¿½ğŸš€ Quick Start

### Prerequisites

- **Python**: 3.9+
- **CUDA**: 11.8 (optional but recommended)
- **Memory**: 8GB+ RAM
- **Storage**: 2GB+ for datasets and models

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/bachnguyen0175/L-CoGNN.git
cd L-CoGNN

# Install dependencies
pip install -r requirements.txt

# Verify installation
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
```

### 2. Quick Training Pipeline

```bash
# Navigate to scripts directory
cd code/scripts

# Option 1: Run complete pipeline automatically
bash run_all.sh acm

# Option 2: Run individual stages
bash 1_train_teacher.sh        # Stage 1: Train main teacher (~30-60 min)
bash 2_train_middle_teacher.sh # Stage 2: Train augmentation teacher (~15-30 min)
bash 3_train_student.sh        # Stage 3: Train student with dual teachers (~20-40 min)
bash 4_evaluate.sh             # Stage 4: Comprehensive evaluation (~5 min)
```

### 3. Training Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Training Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Stage 1: Main Teacher                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚  â”‚  Original Graph      â”‚ â”€â”€â–º Main Teacher (100%)          â”‚
â”‚  â”‚  - PAP, PSP paths    â”‚     - Knowledge distillation     â”‚
â”‚  â”‚  - Contrastive loss  â”‚     - Clean representations      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                                                             â”‚
â”‚  Stage 2: Augmentation Teacher (Independent)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚  â”‚  Augmented Graphs    â”‚ â”€â”€â–º Augmentation Teacher (100%)  â”‚
â”‚  â”‚  - Structure masking â”‚     - Robustness guidance        â”‚
â”‚  â”‚  - Feature dropout   â”‚     - Augmentation patterns      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                                                             â”‚
â”‚  Stage 3: Student (Dual-Teacher Learning)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚         Main Teacher (frozen)                 â”‚          â”‚
â”‚  â”‚               â†“ KD Loss                       â”‚          â”‚
â”‚  â”‚         Student Model (50%)  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚               â†‘ Augmentation                 â”‚      â”‚   â”‚
â”‚  â”‚               â†‘ Alignment Loss                â”‚      â”‚   â”‚
â”‚  â”‚   Augmentation Teacher (frozen)               â”‚      â”‚   â”‚
â”‚  â”‚                                               â”‚      â”‚   â”‚
â”‚  â”‚   + Student Contrastive Loss                  â”‚      â”‚   â”‚
â”‚  â”‚   + Link Reconstruction Loss (optional)       â”‚      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚   â”‚
â”‚                                                         â”‚   â”‚
â”‚  Total Loss = Student Loss                              â”‚   â”‚
â”‚             + main_distill_weight * KD Loss             â”‚   â”‚
â”‚             + augmentation_weight * Alignment Loss      â”‚   â”‚
â”‚             + link_recon_weight * Link Loss             â”‚   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Expected Output

```
âœ… Complete pipeline completed successfully for acm!

ğŸ“ Generated Models:
   - teacher_heco_acm.pkl          (1.2M params, baseline)
   - middle_teacher_heco_acm.pkl   (1.2M params, augmentation expert)
   - student_heco_acm.pkl          (600K params, 50% compressed)

ğŸ¯ Model Comparison:
   Main Teacher:        100% parameters, Baseline performance
   Augmentation Teacher: 100% parameters (same architecture, different data)
   Student:             50% parameters, ~95% performance retention
```

## ğŸ“ Project Structure

```
L-CoGNN/
â”œâ”€â”€ ğŸ“‹ README.md                     # This file
â”œâ”€â”€ âš™ï¸ requirements.txt             # Dependencies
â”œâ”€â”€ ğŸ main.py                      # Entry point (future CLI)
â”‚
â”œâ”€â”€ ğŸ§  code/models/                 # Neural Network Models
â”‚   â”œâ”€â”€ kd_heco.py                  # Core architectures
â”‚   â”‚   â”œâ”€â”€ MyHeCo                  # Main teacher model
â”‚   â”‚   â”œâ”€â”€ AugmentationTeacher     # Augmentation teacher (same size)
â”‚   â”‚   â”œâ”€â”€ StudentMyHeCo           # Compressed student (50%)
â”‚   â”‚   â””â”€â”€ DualTeacherKD           # KD framework coordinator
â”‚   â”œâ”€â”€ contrast.py                 # Contrastive learning module
â”‚   â”œâ”€â”€ sc_encoder.py               # Schema-level attention encoder
â”‚   â””â”€â”€ kd_params.py                # Model & training configurations
â”‚
â”œâ”€â”€ ğŸ“ code/training/               # Training Scripts
â”‚   â”œâ”€â”€ pretrain_teacher.py         # Stage 1: Main teacher
â”‚   â”œâ”€â”€ train_middle_teacher.py     # Stage 2: Augmentation teacher
â”‚   â”œâ”€â”€ train_student.py            # Stage 3: Dual-teacher student
â”‚   â””â”€â”€ hetero_augmentations.py     # Graph augmentation techniques
â”‚
â”œâ”€â”€ ğŸ“Š code/evaluation/             # Evaluation Tools
â”‚   â”œâ”€â”€ comprehensive_evaluation.py # Multi-task evaluation
â”‚   â””â”€â”€ evaluate_kd.py              # KD-specific metrics
â”‚
â”œâ”€â”€ ğŸ”§ code/utils/                  # Utility Functions
â”‚   â”œâ”€â”€ load_data.py                # Data loading
â”‚   â”œâ”€â”€ evaluate.py                 # Evaluation metrics
â”‚   â””â”€â”€ logreg.py                   # Logistic regression
â”‚
â”œâ”€â”€ ğŸš€ code/scripts/                # Shell Scripts
â”‚   â”œâ”€â”€ 1_train_teacher.sh          # Train main teacher
â”‚   â”œâ”€â”€ 2_train_middle_teacher.sh   # Train augmentation teacher
â”‚   â”œâ”€â”€ 3_train_student.sh          # Train student (dual-teacher)
â”‚   â”œâ”€â”€ 4_evaluate.sh               # Comprehensive evaluation
â”‚   â””â”€â”€ run_all.sh                  # Complete pipeline
â”‚
â”œâ”€â”€ ğŸ§ª code/experiments/            # Experiment Configurations
â”‚   â”œâ”€â”€ configs/                    # YAML configurations
â”‚   â””â”€â”€ ablation/                   # Ablation studies
â”‚
â”œâ”€â”€ ğŸ“ data/                        # Datasets
â”‚   â”œâ”€â”€ acm/                        # ACM dataset
â”‚   â”œâ”€â”€ dblp/                       # DBLP dataset
â”‚   â”œâ”€â”€ aminer/                     # AMiner dataset
â”‚   â””â”€â”€ freebase/                   # Freebase dataset
â”‚
â”œâ”€â”€ ğŸ“ˆ results/                     # Model Checkpoints
â”‚   â”œâ”€â”€ teacher_heco_*.pkl          # Main teacher (100%)
â”‚   â”œâ”€â”€ middle_teacher_heco_*.pkl   # Augmentation teacher (100%)
â”‚   â””â”€â”€ student_heco_*.pkl          # Student (50%)
â”‚
â””â”€â”€ ğŸ§ª code/tests/                  # Unit Tests
    â””â”€â”€ test_imports.py             # Import validation
```

## ğŸ¯ Usage Guide

### Training Individual Models

#### 1. Main Teacher Training (Stage 1)
```bash
cd code/scripts
bash 1_train_teacher.sh

# Or with custom parameters
cd code/training
python pretrain_teacher.py acm \
    --hidden_dim=64 \
    --nb_epochs=1000 \
    --lr=0.0008 \
    --gpu 0
```

**What happens**: Trains the main teacher on original graph data using contrastive learning.

#### 2. Augmentation Teacher Training (Stage 2)
```bash
bash 2_train_middle_teacher.sh

# Or with custom parameters
cd code/training
python train_middle_teacher.py acm \
    --hidden_dim=64 \
    --nb_epochs=100 \
    --lr=0.0008 \
    --gpu 0
```

**What happens**: Trains augmentation teacher independently on augmented graphs. **No compression** - same architecture as main teacher but learns robust patterns from data augmentation.

#### 3. Student Training with Dual Teachers (Stage 3)
```bash
bash 3_train_student.sh

# Or with custom parameters
cd code/training
python train_student.py acm \
    --teacher_model_path ../../results/teacher_heco_acm.pkl \
    --middle_teacher_path ../../results/middle_teacher_heco_acm.pkl \
    --student_compression_ratio=0.5 \
    --stage2_epochs=100 \
    --lr=0.0008 \
    --gpu 0
```

**What happens**: 
- Loads **both frozen teachers** (main + augmentation)
- Trains compressed student (50% parameters) with:
  - **KD Loss**: Learn from main teacher's representations
  - **Augmentation Alignment**: Learn robustness from augmentation teacher
  - **Student Contrastive Loss**: Self-supervised learning
  - **Link Reconstruction** (optional): Explicit edge modeling

### Evaluation and Analysis

#### Comprehensive Multi-Task Evaluation
```bash
bash 4_evaluate.sh

# Or directly:
cd code/evaluation
python comprehensive_evaluation.py \
    --dataset acm \
    --teacher_path ../../results/teacher_heco_acm.pkl \
    --middle_teacher_path ../../results/middle_teacher_heco_acm.pkl \
    --student_path ../../results/student_heco_acm.pkl
```

**Evaluates**:
- âœ… Node Classification (Macro-F1, Micro-F1, Accuracy)
- âœ… Link Prediction (AUC, AP)
- âœ… Node Clustering (NMI, ARI)
- âœ… Model Compression Metrics
- âœ… Inference Speed Comparison

#### KD-Specific Metrics
```bash
cd code/evaluation
python evaluate_kd.py \
    --dataset acm \
    --teacher_model_path ../../results/teacher_heco_acm.pkl \
    --student_model_path ../../results/student_heco_acm.pkl
```

**Analyzes**:
- Knowledge transfer quality
- Representation similarity
- Layer-wise distillation effectiveness

### Key Training Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--student_compression_ratio` | 0.5 | Student size relative to teacher (0.5 = 50%) |
| `--main_distill_weight` | 1.0 | Weight for main teacher KD loss |
| `--augmentation_weight` | 0.5 | Weight for augmentation alignment |
| `--link_recon_weight` | 0.1 | Weight for link reconstruction |
| `--use_kd_loss` | True | Enable/disable KD from main teacher |
| `--use_augmentation_alignment_loss` | True | Enable/disable augmentation guidance |
| `--use_link_recon_loss` | False | Enable/disable link reconstruction |
| `--use_student_contrast_loss` | True | Enable/disable student self-learning |

## ğŸ“Š Datasets

### Supported Datasets

| Dataset | Nodes | Edges | Node Types | Tasks |
|---------|-------|-------|------------|-------|
| **ACM** | 4,019 papers<br>7,167 authors<br>60 subjects | PAP, PSP | Paper, Author, Subject | Classification, Link Prediction, Clustering |
| **DBLP** | 4,057 papers<br>14,328 authors<br>7,723 conferences<br>20 terms | PAP, PCP, PTP | Paper, Author, Conference, Term | Classification, Link Prediction, Clustering |
| **AMiner** | 6,564 papers<br>13,329 authors<br>35,890 references | PAP, PRP | Paper, Author, Reference | Classification, Link Prediction, Clustering |
| **Freebase** | Multi-relational | Multiple | Multiple | Classification, Link Prediction, Clustering |

### Data Format

Each dataset contains:
- **Feature files**: `*_feat.npz` (node features)
- **Graph files**: `*.npz` (adjacency matrices)
- **Labels**: `labels.npy` (node labels)
- **Splits**: `train_*.npy`, `val_*.npy`, `test_*.npy`

## âš™ï¸ Configuration

### Model Architecture Configuration

```yaml
# Example: ACM dataset configuration
dataset: acm
type_num: [4019, 7167, 60]  # Node counts per type
nei_num: 2                   # Number of neighbor types (for schema encoder)

model:
  hidden_dim: 64             # Hidden dimension for teacher & augmentation teacher
  student_compression_ratio: 0.5  # Student = 32 dim (50% of 64)
  feat_drop: 0.3            # Feature dropout
  attn_drop: 0.5            # Attention dropout
  tau: 0.8                  # Temperature for contrastive learning
  lam: 0.5                  # Balance parameter for contrastive loss

training:
  teacher:
    epochs: 1000            # Main teacher training epochs
    lr: 0.0008             # Learning rate
    patience: 50           # Early stopping patience
    
  middle_teacher:
    epochs: 100            # Augmentation teacher epochs
    lr: 0.0008
    patience: 30
    
  student:
    epochs: 100            # Student training epochs
    lr: 0.0008
    main_distill_weight: 1.0      # KD loss weight
    augmentation_weight: 0.5      # Augmentation alignment weight
    link_recon_weight: 0.1        # Link reconstruction weight (optional)
    
augmentation:
  feature_masking: 0.2     # Feature masking ratio
  edge_perturbation: 0.1   # Edge perturbation ratio
  metapath_sampling: True  # Enable metapath-based augmentation
```

### Hardware Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| **GPU** | GTX 1060 (6GB) | RTX 3050+ (8GB+) |
| **RAM** | 8GB | 16GB+ |
| **CUDA** | 11.0+ | 11.8 |
| **Storage** | 2GB | 5GB+ |

## ğŸ”§ Development

### Running Tests

```bash
# Test import structure
python code/tests/test_imports.py

# Run all tests (future)
python -m pytest code/tests/
```

### Code Style

```bash
# Format code
black code/

# Lint code  
flake8 code/
```

### Adding New Datasets

1. Create dataset directory in `data/`
2. Add configuration in `experiments/configs/`
3. Update `utils/load_data.py` if needed
4. Test with existing pipeline

## ğŸ“ˆ Performance Benchmarks

### Node Classification Results

| Dataset | Teacher | Middle Teacher | Student | Retention |
|---------|---------|----------------|---------|-----------|
| ACM | 89.2% | 87.8% (-1.4%) | 85.1% (-4.1%) | 95.4% |
| DBLP | 91.5% | 89.9% (-1.6%) | 87.2% (-4.3%) | 95.3% |
| AMiner | 88.7% | 87.1% (-1.6%) | 84.8% (-3.9%) | 95.6% |

### Model Size Comparison

| Model | Parameters | Memory (MB) | Inference Time (ms) | Role |
|-------|------------|-------------|---------------------|------|
| Teacher | 1.2M | 45.3 | 12.4 | Main knowledge source (trained on original data) |
| Middle Teacher | 1.2M | 45.3 | 12.4 | Augmentation expert (trained on augmented data) |
| Student | 600K | 22.7 | 6.2 | Compressed model (50% reduction) |


## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Development Setup

```bash
# Clone and setup development environment
git clone https://github.com/bachnguyen0175/L-CoGNN.git
cd L-CoGNN
pip install -r requirements.txt
pip install -e .  # Editable install

# Run tests
python -m pytest
```

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“š Citation

If you use this code in your research, please cite:

```bibtex
@article{l_cognn2024,
  title={L-CoGNN: Knowledge Distillation for Heterogeneous Graph Representation Learning},
  author={Nguyen, Bach and Team},
  journal={arXiv preprint arXiv:2024.xxxxx},
  year={2024}
}
```

## ğŸ™ Acknowledgments

- **PyTorch Team** for the excellent deep learning framework
- **DGL Team** for graph neural network utilities  
- **Research Community** for heterogeneous graph learning advances
- **Contributors** who helped improve this project

## ğŸ“ Contact

- **Author**: Bach Nguyen
- **Email**: [bachnguyen0175@email.com]
- **GitHub**: [@bachnguyen0175](https://github.com/bachnguyen0175)
- **Project**: [L-CoGNN Repository](https://github.com/bachnguyen0175/L-CoGNN)

---

<div align="center">

**â­ If you find this project useful, please consider giving it a star! â­**

Made with â¤ï¸ for the Graph Neural Network community

</div>