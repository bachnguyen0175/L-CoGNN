# So SÃ¡nh Chi Tiáº¿t: HeCo vs CODE_SAMPLE

## Tá»•ng Quan
- **HeCo**: Model gá»‘c (original baseline)
- **CODE_SAMPLE**: Framework Knowledge Distillation vá»›i kiáº¿n trÃºc Teacher-Student Ä‘a cáº¥p

---

## 1. KIáº¾N TRÃšC TEACHER MODEL

### 1.1. HeCo (Model Gá»‘c)
```python
class HeCo(nn.Module):
    def __init__(self, hidden_dim, feats_dim_list, feat_drop, attn_drop, 
                 P, sample_rate, nei_num, tau, lam):
        # ÄÆ¡n giáº£n, chá»‰ cÃ³ 1 model duy nháº¥t
        self.fc_list = nn.ModuleList([...])  # Feature projection
        self.mp = Mp_encoder(P, hidden_dim, attn_drop)  # Meta-path encoder
        self.sc = Sc_encoder(hidden_dim, sample_rate, nei_num, attn_drop)  # Schema encoder
        self.contrast = Contrast(hidden_dim, tau, lam)  # Contrastive learning
```

**Äáº·c Ä‘iá»ƒm:**
- âœ… **1 model duy nháº¥t** - khÃ´ng cÃ³ phÃ¢n cáº¥p Teacher-Student
- âœ… Há»c trá»±c tiáº¿p trÃªn dá»¯ liá»‡u gá»‘c
- âœ… Architecture Ä‘Æ¡n giáº£n, dá»… hiá»ƒu

### 1.2. CODE_SAMPLE (Framework Knowledge Distillation)

#### A. Teacher Model ChÃ­nh (MyHeCo)
```python
class MyHeCo(nn.Module):
    def __init__(self, hidden_dim, feats_dim_list, feat_drop, attn_drop, 
                 P, sample_rate, nei_num, tau, lam, **kwargs):
        # TÆ°Æ¡ng tá»± HeCo nhÆ°ng cÃ³ thÃªm tÃ­nh nÄƒng
        self.fc_list = nn.ModuleList([...])
        self.mp = myMp_encoder(P, hidden_dim, attn_drop)
        self.sc = mySc_encoder(hidden_dim, sample_rate, nei_num, attn_drop)
        self.contrast = Contrast(hidden_dim, tau, lam)
```

**Äáº·c Ä‘iá»ƒm:**
- âœ… **Base teacher** - há»c trÃªn dá»¯ liá»‡u gá»‘c
- âœ… Cáº¥u trÃºc giá»‘ng HeCo 95%
- âœ… GCN layer há»— trá»£ cáº£ sparse vÃ  dense matrices

#### B. Augmentation Teacher (Middle Teacher)
```python
class AugmentationTeacher(nn.Module):
    def __init__(self, feats_dim_list, hidden_dim, attn_drop, feat_drop, 
                 P, sample_rate, nei_num, tau, lam, augmentation_config=None):
        # Specialized teacher cho augmented data
        self.fc_list = nn.ModuleList([...])
        self.mp = myMp_encoder(P, self.expert_dim, attn_drop)
        self.sc = mySc_encoder(self.expert_dim, sample_rate, nei_num, attn_drop)
        self.contrast = Contrast(self.expert_dim, tau, lam)
        
        # THÃŠM: Pipeline augmentation
        self.augmentation_pipeline = HeteroAugmentationPipeline(...)
```

**Äáº·c Ä‘iá»ƒm:**
- ğŸ”¥ **KHÃC BIá»†T Lá»šN** - KhÃ´ng cÃ³ trong HeCo gá»‘c
- ğŸ”¥ Há»c trÃªn **dá»¯ liá»‡u augmented** (edge drop, feature mask, node drop, etc.)
- ğŸ”¥ Cung cáº¥p augmentation guidance cho student
- ğŸ”¥ CÃ³ thÃªm `augmentation_pipeline` Ä‘á»ƒ táº¡o augmented graphs

**Káº¿t luáº­n Teacher Model:**
| TiÃªu chÃ­ | HeCo | CODE_SAMPLE |
|----------|------|-------------|
| Sá»‘ lÆ°á»£ng Teacher | 1 model | 2 teachers (Base + Augmentation) |
| Dá»¯ liá»‡u huáº¥n luyá»‡n | Original data | Original + Augmented data |
| Má»¥c Ä‘Ã­ch | Direct learning | Knowledge Distillation Framework |
| Äá»™ phá»©c táº¡p | ÄÆ¡n giáº£n | Phá»©c táº¡p hÆ¡n |

---

## 2. META-PATH ENCODER (CÃ¡ch Há»c Meta-path)

### 2.1. HeCo: Mp_encoder
```python
class Mp_encoder(nn.Module):
    def __init__(self, P, hidden_dim, attn_drop):
        self.node_level = nn.ModuleList([GCN(hidden_dim, hidden_dim) for _ in range(P)])
        self.att = Attention(hidden_dim, attn_drop)
    
    def forward(self, h, mps):
        embeds = []
        for i in range(self.P):
            embeds.append(self.node_level[i](h, mps[i]))  # GCN cho má»—i meta-path
        z_mp = self.att(embeds)  # Semantic-level attention
        return z_mp
```

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
1. **Node-level GCN**: Má»—i meta-path Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi 1 GCN layer
2. **Semantic Attention**: Káº¿t há»£p cÃ¡c meta-path embeddings qua attention
3. **ÄÆ¡n giáº£n, hiá»‡u quáº£**

### 2.2. CODE_SAMPLE: myMp_encoder
```python
class myMp_encoder(nn.Module):
    def __init__(self, P, hidden_dim, attn_drop):
        self.P = P
        self.node_level = nn.ModuleList([GCN(hidden_dim, hidden_dim) for _ in range(P)])
        self.att = Attention(hidden_dim, attn_drop)
    
    def forward(self, h, mps):
        embeds = []
        for i in range(self.P):
            embeds.append(self.node_level[i](h, mps[i]))
        z_mp = self.att(embeds)
        return z_mp
```

**So sÃ¡nh:**
| KhÃ­a cáº¡nh | HeCo | CODE_SAMPLE |
|-----------|------|-------------|
| Architecture | GCN + Attention | GCN + Attention (giá»‘ng 100%) |
| Node-level encoding | âœ… GCN layers | âœ… GCN layers (identical) |
| Semantic-level fusion | âœ… Attention mechanism | âœ… Attention mechanism (identical) |
| KhÃ¡c biá»‡t | - | CÃ³ improved GCN vá»›i sparse/dense handling |

### KhÃ¡c biá»‡t chÃ­nh trong GCN Layer:

**HeCo GCN (ÄÆ¡n giáº£n):**
```python
def forward(self, seq, adj):
    seq_fts = self.fc(seq)
    out = torch.spmm(adj, seq_fts)  # Chá»‰ há»— trá»£ sparse matrix
    if self.bias is not None:
        out += self.bias
    return self.act(out)
```

**CODE_SAMPLE GCN (Cáº£i tiáº¿n):**
```python
def forward(self, seq, adj):
    seq_fts = self.fc(seq)
    
    # âœ… Há»— trá»£ cáº£ sparse vÃ  dense matrices
    if hasattr(adj, 'is_sparse') and adj.is_sparse:
        out = torch.sparse.mm(adj, seq_fts)
    else:
        out = torch.mm(adj, seq_fts)
    
    # âœ… Xá»­ lÃ½ dimension mismatches
    # âœ… Error handling tá»‘t hÆ¡n
    
    if self.bias is not None:
        out += self.bias
    return self.act(out)
```

**Káº¿t luáº­n Meta-path Learning:**
- **Ã tÆ°á»Ÿng cá»‘t lÃµi: GIá»NG 100%** - Cáº£ 2 Ä‘á»u dÃ¹ng GCN + Semantic Attention
- **Implementation: CODE_SAMPLE cáº£i tiáº¿n** - Robust hÆ¡n vá»›i sparse/dense matrices
- **Vá» máº·t lÃ½ thuyáº¿t: KHÃ”NG CÃ“ KHÃC BIá»†T**

---

## 3. SCHEMA-LEVEL CONTRAST (SC Encoder)

### 3.1. HeCo: Sc_encoder
```python
class Sc_encoder(nn.Module):
    def __init__(self, hidden_dim, sample_rate, nei_num, attn_drop):
        # Intra-type attention (trong cÃ¹ng 1 node type)
        self.intra = nn.ModuleList([intra_att(hidden_dim, attn_drop) for _ in range(nei_num)])
        # Inter-type attention (giá»¯a cÃ¡c node types)
        self.inter = inter_att(hidden_dim, attn_drop)
        self.sample_rate = sample_rate
        self.nei_num = nei_num
    
    def forward(self, nei_h, nei_index):
        embeds = []
        for i in range(self.nei_num):
            # Sample neighbors
            sele_nei = [np.random.choice(...) for per_node_nei in nei_index[i]]
            sele_nei = torch.cat(sele_nei, dim=0).cuda()
            
            # Intra-type aggregation
            one_type_emb = F.elu(self.intra[i](sele_nei, nei_h[i + 1], nei_h[0]))
            embeds.append(one_type_emb)
        
        # Inter-type aggregation
        z_mc = self.inter(embeds)
        return z_mc
```

### 3.2. CODE_SAMPLE: mySc_encoder
```python
class mySc_encoder(nn.Module):
    def __init__(self, hidden_dim, sample_rate, nei_num, attn_drop):
        self.intra = nn.ModuleList([intra_att(hidden_dim, attn_drop) for _ in range(nei_num)])
        self.inter = inter_att(hidden_dim, attn_drop)
        self.sample_rate = sample_rate
        self.nei_num = nei_num
    
    def forward(self, nei_h, nei_index):
        embeds = []
        for i in range(self.nei_num):
            # GIá»NG Há»†T HeCo: neighbor sampling
            sele_nei = [np.random.choice(...) for per_node_nei in nei_index[i]]
            
            # KHÃC BIá»†T NHá»: .to(device) thay vÃ¬ .cuda()
            sele_nei = torch.cat(sele_nei, dim=0).to(nei_h[0].device)
            
            one_type_emb = F.elu(self.intra[i](sele_nei, nei_h[i + 1], nei_h[0]))
            embeds.append(one_type_emb)
        
        z_mc = self.inter(embeds)
        return z_mc
```

### Kiáº¿n trÃºc SC Encoder (Giá»‘ng há»‡t):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SCHEMA-LEVEL CONTRAST               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  Node Type 1    Node Type 2    Node Type 3 â”‚
â”‚       â”‚              â”‚              â”‚       â”‚
â”‚       â–¼              â–¼              â–¼       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Intra-  â”‚   â”‚ Intra-  â”‚   â”‚ Intra-  â”‚  â”‚ <- Attention trong cÃ¹ng type
â”‚  â”‚  Att 1  â”‚   â”‚  Att 2  â”‚   â”‚  Att 3  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚              â”‚              â”‚       â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                     â”‚                       â”‚
â”‚                     â–¼                       â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚              â”‚  Inter-Att  â”‚               â”‚ <- Attention giá»¯a cÃ¡c types
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                     â”‚                       â”‚
â”‚                     â–¼                       â”‚
â”‚              Schema Embedding              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**So sÃ¡nh:**
| KhÃ­a cáº¡nh | HeCo | CODE_SAMPLE |
|-----------|------|-------------|
| Intra-type attention | âœ… CÃ³ | âœ… CÃ³ (giá»‘ng 100%) |
| Inter-type attention | âœ… CÃ³ | âœ… CÃ³ (giá»‘ng 100%) |
| Neighbor sampling | âœ… Random sampling | âœ… Random sampling (giá»‘ng) |
| Device handling | `.cuda()` | `.to(device)` (flexible hÆ¡n) |
| Softmax dim | `dim=None` (cÃ³ bug) | `dim=-1` (fixed) |

**Káº¿t luáº­n SC Encoder:**
- **Ã tÆ°á»Ÿng: GIá»NG 100%** - Cáº£ 2 Ä‘á»u dÃ¹ng Intra + Inter attention
- **Implementation: 99% giá»‘ng nhau**
- **KhÃ¡c biá»‡t duy nháº¥t: CODE_SAMPLE fix má»™t vÃ i bugs nhá»**

---

## 4. SCHEMA PATH (Meta-path vs Schema trong CODE_SAMPLE)

### 4.1. Trong HeCo

HeCo sá»­ dá»¥ng **Meta-path** vÃ  **Network Schema** nhÆ° sau:

**Meta-path (MP):**
- LÃ  cÃ¡c path cá»¥ thá»ƒ trong heterogeneous graph
- VD: Paper-Author-Paper (PAP), Paper-Subject-Paper (PSP)
- ÄÆ°á»£c encode qua **Mp_encoder**

**Network Schema (SC):**
- LÃ  neighbor structure cá»§a cÃ¡c node types khÃ¡c nhau
- VD: Author neighbors, Subject neighbors
- ÄÆ°á»£c encode qua **Sc_encoder**

### 4.2. Trong CODE_SAMPLE

**Meta-path (giá»‘ng HeCo):**
```python
# Trong training
z_mp = self.mp(h_all[0], mps)  # mps = list of meta-path adjacency matrices
```

**Schema-level (giá»‘ng HeCo):**
```python
# Trong training
z_sc = self.sc(h_all, nei_index)  # nei_index = neighbor indices by type
```

**Káº¿t luáº­n:**
| Concept | HeCo | CODE_SAMPLE |
|---------|------|-------------|
| Meta-path | âœ… PAP, PSP, etc. | âœ… PAP, PSP, etc. (GIá»NG) |
| Schema-level | âœ… Network schema | âœ… Network schema (GIá»NG) |
| CÃ¡ch encode MP | Mp_encoder | myMp_encoder (GIá»NG) |
| CÃ¡ch encode SC | Sc_encoder | mySc_encoder (GIá»NG) |

**CODE_SAMPLE KHÃ”NG thay Ä‘á»•i cÃ¡ch hiá»ƒu vá» Meta-path hay Schema path!**

---

## 5. CONTRASTIVE LEARNING

### 5.1. HeCo Contrast Module
```python
class Contrast(nn.Module):
    def __init__(self, hidden_dim, tau, lam):
        self.proj = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ELU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        self.tau = tau  # Temperature
        self.lam = lam  # Balance parameter
    
    def forward(self, z_mp, z_sc, pos):
        z_proj_mp = self.proj(z_mp)
        z_proj_sc = self.proj(z_sc)
        
        # Similarity matrix
        matrix_mp2sc = self.sim(z_proj_mp, z_proj_sc)
        matrix_sc2mp = matrix_mp2sc.t()
        
        # Contrastive loss
        lori_mp = -torch.log(matrix_mp2sc.mul(pos.to_dense()).sum(dim=-1)).mean()
        lori_sc = -torch.log(matrix_sc2mp.mul(pos.to_dense()).sum(dim=-1)).mean()
        
        return self.lam * lori_mp + (1 - self.lam) * lori_sc
```

### 5.2. CODE_SAMPLE Contrast Module
```python
class Contrast(nn.Module):
    # GIá»NG Há»†T HeCo - copy 100%
    # Chá»‰ khÃ¡c: convert pos.to_dense() 1 láº§n thay vÃ¬ 2 láº§n (optimization)
    
    def forward(self, z_mp, z_sc, pos):
        z_proj_mp = self.proj(z_mp)
        z_proj_sc = self.proj(z_sc)
        matrix_mp2sc = self.sim(z_proj_mp, z_proj_sc)
        matrix_sc2mp = matrix_mp2sc.t()
        
        pos_dense = pos.to_dense()  # âœ… Optimize: convert once
        
        matrix_mp2sc = matrix_mp2sc/(torch.sum(matrix_mp2sc, dim=1).view(-1, 1) + 1e-8)
        lori_mp = -torch.log(matrix_mp2sc.mul(pos_dense).sum(dim=-1)).mean()

        matrix_sc2mp = matrix_sc2mp / (torch.sum(matrix_sc2mp, dim=1).view(-1, 1) + 1e-8)
        lori_sc = -torch.log(matrix_sc2mp.mul(pos_dense).sum(dim=-1)).mean()
        
        return self.lam * lori_mp + (1 - self.lam) * lori_sc
```

**Káº¿t luáº­n Contrastive Learning:**
- **HoÃ n toÃ n GIá»NG NHAU** - 100%
- CODE_SAMPLE chá»‰ cÃ³ optimization nhá» (convert sparse to dense 1 láº§n thay vÃ¬ 2)

---

## 6. TÃ“M Táº®T Tá»”NG QUAN

### 6.1. Äiá»ƒm Giá»‘ng Nhau (95%)

| Component | HeCo | CODE_SAMPLE | Tá»· lá»‡ giá»‘ng |
|-----------|------|-------------|-------------|
| **Meta-path Encoder** | Mp_encoder | myMp_encoder | **100%** |
| **Schema Encoder** | Sc_encoder | mySc_encoder | **99%** |
| **Contrastive Learning** | Contrast | Contrast | **100%** |
| **GCN Architecture** | Basic GCN | Enhanced GCN | **95%** |
| **Attention Mechanism** | Semantic + Type-level | Semantic + Type-level | **100%** |
| **Meta-path concept** | PAP, PSP, etc. | PAP, PSP, etc. | **100%** |
| **Schema-level concept** | Network schema | Network schema | **100%** |

### 6.2. Äiá»ƒm KhÃ¡c Nhau (5%)

| KhÃ­a cáº¡nh | HeCo | CODE_SAMPLE |
|-----------|------|-------------|
| **Sá»‘ Teacher Models** | 1 model duy nháº¥t | 2 teachers (Base + Augmentation) |
| **Augmentation** | âŒ KhÃ´ng cÃ³ | âœ… HeteroAugmentationPipeline |
| **Knowledge Distillation** | âŒ KhÃ´ng cÃ³ | âœ… Dual-teacher KD framework |
| **Student Model** | âŒ KhÃ´ng cÃ³ | âœ… Lightweight student |
| **GCN Implementation** | Chá»‰ sparse | Cáº£ sparse & dense |
| **Training Pipeline** | Single-stage | Multi-stage (Teacher â†’ Student) |
| **Má»¥c Ä‘Ã­ch** | Direct learning | Model compression & KD |

### 6.3. Káº¿t Luáº­n ChÃ­nh

ğŸ¯ **Vá»€ TEACHER MODEL:**
- **MyHeCo (Base Teacher)**: Giá»‘ng HeCo **~98%**
- **AugmentationTeacher**: KHÃC BIá»†T Lá»šN - khÃ´ng cÃ³ trong HeCo

ğŸ¯ **Vá»€ CÃCH Há»ŒC META-PATH:**
- **GIá»NG 100%**: Cáº£ 2 Ä‘á»u dÃ¹ng GCN + Semantic Attention
- CODE_SAMPLE chá»‰ improve implementation

ğŸ¯ **Vá»€ CÃCH Há»ŒC SCHEMA-LEVEL:**
- **GIá»NG 100%**: Cáº£ 2 Ä‘á»u dÃ¹ng Intra + Inter attention
- CODE_SAMPLE chá»‰ fix bugs nhá»

ğŸ¯ **Vá»€ SCHEMA PATH:**
- **KHÃ”NG CÃ“ Sá»° KHÃC BIá»†T** trong khÃ¡i niá»‡m meta-path vÃ  schema
- Cáº£ 2 hiá»ƒu vÃ  sá»­ dá»¥ng giá»‘ng nhau

---

## 7. FLOW DIAGRAM SO SÃNH

### HeCo (Original):
```
Input Features
      â†“
Feature Projection (fc_list)
      â†“
  â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”
  â†“        â†“
Mp_encoder  Sc_encoder
  (Meta-path) (Schema)
  â†“        â†“
  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â†“
  Contrast Loss
      â†“
  Final Embeddings
```

### CODE_SAMPLE (KD Framework):
```
STAGE 1: Train Base Teacher (MyHeCo)
Input Features â†’ Feature Projection â†’ Mp/Sc Encoders â†’ Contrast Loss

STAGE 2: Train Augmentation Teacher
Augmented Graph â†’ Feature Projection â†’ Mp/Sc Encoders â†’ Contrast Loss

STAGE 3: Train Student with Dual Teachers
Input â†’ Student Model
         â†“
    Knowledge Transfer â† Base Teacher Knowledge
         â†“
    Knowledge Transfer â† Augmentation Teacher Knowledge
         â†“
    Lightweight Embeddings
```

---

## 8. Káº¾T LUáº¬N CUá»I CÃ™NG

### âœ… CODE_SAMPLE cÃ³ GIá»NG HeCo khÃ´ng?

**CÃ“ - 95% giá»‘ng vá» core architecture:**
- Meta-path learning: **GIá»NG 100%**
- Schema-level contrast: **GIá»NG 100%**
- Contrastive learning: **GIá»NG 100%**
- Base teacher model (MyHeCo): **GIá»NG 98%**

### âŒ CODE_SAMPLE cÃ³ gÃ¬ KHÃC?

**5% khÃ¡c biá»‡t quan trá»ng:**
1. **ThÃªm Augmentation Teacher** - há»c trÃªn augmented graphs
2. **ThÃªm Student Model** - lightweight distilled model
3. **Knowledge Distillation Framework** - dual-teacher distillation
4. **Multi-stage training** - teacher â†’ student pipeline

### ğŸ¯ TÃ³m láº¡i:

**CODE_SAMPLE = HeCo (95%) + Knowledge Distillation Framework (5%)**

- Vá» **lÃ½ thuyáº¿t meta-path vÃ  schema**: KHÃ”NG KHÃC
- Vá» **implementation chi tiáº¿t**: CÃ“ Cáº¢I TIáº¾N NHá»
- Vá» **má»¥c Ä‘Ã­ch sá»­ dá»¥ng**: KHÃC HOÃ€N TOÃ€N (KD framework vs direct learning)

**CODE_SAMPLE giá»¯ nguyÃªn Ã½ tÆ°á»Ÿng cá»‘t lÃµi cá»§a HeCo, nhÆ°ng má»Ÿ rá»™ng thÃ nh framework Knowledge Distillation vá»›i 2 teachers vÃ  1 student model!**
