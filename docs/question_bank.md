# L-CoGNN / KD-HGRL Question Bank

Cáº­p nháº­t: 2025-10-28

## ğŸ¯ Má»¥c ÄÃ­ch
Táº­p há»£p cÃ¡c cÃ¢u há»i thÆ°á»ng gáº·p (FAQ) vÃ  bá»™ tráº£ lá»i á»Ÿ nhiá»u cáº¥p Ä‘á»™ (ngáº¯n gá»n, chuáº©n bá»‹ thuyáº¿t trÃ¬nh, ká»¹ thuáº­t chi tiáº¿t, há»c thuáº­t) Ä‘á»ƒ dÃ¹ng khi viáº¿t bÃ¡o cÃ¡o, tráº£ lá»i reviewer, phá»ng váº¥n hoáº·c trÃ¬nh bÃ y dá»± Ã¡n.

## ğŸ§ª CÃ¡ch Sá»­ Dá»¥ng
- TÃ¬m nhÃ³m chá»§ Ä‘á» phÃ¹ há»£p (Middle Teacher, Distillation, Losses...).
- Chá»n phiÃªn báº£n tráº£ lá»i theo bá»‘i cáº£nh (one-liner, pitch, technical, academic).
- CÃ³ thá»ƒ trÃ­ch dáº«n trá»±c tiáº¿p hoáº·c tinh chá»‰nh thÃªm.

---
## 1. KhÃ¡i Niá»‡m Cá»‘t LÃµi (Core Concepts)
### Q1: L-CoGNN lÃ  gÃ¬?
- One-liner: Framework distillation hai teacher cho Ä‘á»“ thá»‹ dá»‹ thá»ƒ giÃºp nÃ©n mÃ´ hÃ¬nh ~50% mÃ  váº«n giá»¯ ~95% hiá»‡u nÄƒng.
- Pitch: L-CoGNN káº¿t há»£p má»™t teacher chÃ­nh há»c trÃªn dá»¯ liá»‡u sáº¡ch vÃ  má»™t augmentation teacher há»c trÃªn dá»¯ liá»‡u biáº¿n Ä‘á»•i Ä‘á»ƒ huáº¥n luyá»‡n student nÃ©n, cÃ¢n báº±ng giá»¯a Ä‘á»™ chÃ­nh xÃ¡c vÃ  kháº£ nÄƒng chá»‘ng nhiá»…u.
- Technical: Pipeline gá»“m MyHeCo (clean), AugmentationTeacher (robustness), StudentMyHeCo (compressed 50%). Loss tá»•ng: student contrast + KD + augmentation alignment (+ optional link reconstruction). Má»¥c tiÃªu: giáº£m tham sá»‘ nhÆ°ng duy trÃ¬ cháº¥t lÆ°á»£ng biá»ƒu diá»…n cho node classification / link prediction.
- Academic: We propose a dual-teacher distillation paradigm for heterogeneous graphs wherein a primary semantic teacher and an augmentation-driven robustness teacher jointly supervise a compressed student, achieving strong representation retention under structural perturbations.

### Q2: KhÃ¡c gÃ¬ so vá»›i distillation truyá»n thá»‘ng?
- Ngáº¯n: ThÃªm má»™t teacher chuyÃªn vá» robustness thay vÃ¬ chá»‰ cÃ³ teacher chÃ­nh.
- Äáº§y Ä‘á»§: Distillation thÆ°á»ng chá»‰ sao chÃ©p tri thá»©c tá»« mÃ´ hÃ¬nh lá»›n â†’ nhá». á» Ä‘Ã¢y, ta tÃ¡ch tri thá»©c thÃ nh hai miá»n: semantic (clean) vÃ  robustness (augmented). Student há»£p nháº¥t cáº£ hai giÃºp á»•n Ä‘á»‹nh vÃ  bá»n vá»¯ng hÆ¡n.
- Ká»¹ thuáº­t: Thay vÃ¬ H = f_clean(x), thÃªm H_aug = f_aug(T_aug(x')). Alignment loss buá»™c student tá»‘i Æ°u theo hai tÃ­n hiá»‡u cÃ³ trá»ng sá»‘ (main_distill_weight, augmentation_weight). KhÃ´ng nÃ©n teacher thá»© hai Ä‘á»ƒ khÃ´ng máº¥t Ä‘á»™ Ä‘a dáº¡ng embedding augmented.
- Há»c thuáº­t: Conventional KD focuses on soft target or representation imitation. Our dual-teacher setup introduces functionally disentangled supervision signals (semantic vs. perturbation-aware), improving resilience and reducing overfitting under graph structure shifts.

---
## 2. Middle (Augmentation) Teacher
### Q3: Middle teacher dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬?
- One-liner: Cung cáº¥p tÃ­n hiá»‡u robust vÃ  trá»ng sá»‘ cáº¥u trÃºc tá»« meta-path augmented embeddings.
- Pitch: NÃ³ há»c trÃªn embedding Ä‘Æ°á»£c khuáº¿ch Ä‘áº¡i qua meta-path propagation (khÃ´ng thÃªm cáº¡nh má»›i, chá»‰ táº­n dá»¥ng multi-hop connections cÃ³ sáºµn) Ä‘á»ƒ cho student biáº¿t pháº§n nÃ o á»•n Ä‘á»‹nh khi graph biáº¿n thiÃªn.
- Technical: AugmentationTeacher tÃ­nh mp_guidance, sc_guidance, attention_importance, structure_importance tá»« embeddings Ä‘Æ°á»£c augment báº±ng meta-path propagation + low-rank projection + semantic attention. CÃ¡c tensor nÃ y há»— trá»£ student Ä‘iá»u chá»‰nh trá»ng sá»‘ táº­p trung. KhÃ´ng dÃ¹ng Ä‘á»ƒ nÃ©n.
- Academic: The augmentation teacher operates as a robustness oracle, producing structural importance distributions that guide the compressed student toward perturbation-invariant latent subspaces.

### Q4: Táº¡i sao middle teacher khÃ´ng bá»‹ nÃ©n?
- Ngáº¯n: Giá»¯ kÃ­ch thÆ°á»›c Ä‘á»ƒ tá»‘i Ä‘a hÃ³a cháº¥t lÆ°á»£ng tÃ­n hiá»‡u augmentation.
- Äáº§y Ä‘á»§: NÃ©n sáº½ lÃ m giáº£m Ä‘á»™ biá»ƒu Ä‘áº¡t cá»§a embedding augmented, giáº£m hiá»‡u quáº£ alignment. Giá»¯ nguyÃªn kÃ­ch thÆ°á»›c Ä‘áº£m báº£o miá»n robust giÃ u thÃ´ng tin.
- Ká»¹ thuáº­t: expert_dim == hidden_dim; student_dim = hidden_dim * compression_ratio. Distillation chá»‰ Ã¡p vÃ o student, khÃ´ng Ã¡p lÃªn augmentation teacher.
- Há»c thuáº­t: Preserving full representational capacity in the augmentation expert prevents information bottlenecks in robustness transfer and maintains diversity in structural guidance vectors.

### Q5: Náº¿u bá» middle teacher thÃ¬ sao?
- Tráº£ lá»i: Student váº«n há»c Ä‘Æ°á»£c tri thá»©c semantic nhÆ°ng giáº£m kháº£ nÄƒng chá»‘ng nhiá»…u, dá»… overfit meta-path phá»• biáº¿n, kÃ©m tá»•ng quÃ¡t khi graph thay Ä‘á»•i nháº¹.

---
## 3. Student & Compression
### Q6: Student nÃ©n nhÆ° tháº¿ nÃ o?
- Ngáº¯n: Giáº£m 50% hidden dimension qua projection + encoder thu gá»n.
- Technical: student_dim = int(hidden_dim * compression_ratio). Linear layers + mp/sc encoder dÃ¹ng dimension má»›i; thÃªm projection lÃªn teacher_dim cho KD alignment.

### Q7: LÃ½ do chá»n 50%?
- Ngáº¯n: CÃ¢n báº±ng giá»¯a retention vÃ  tá»‘c Ä‘á»™.
- Äáº§y Ä‘á»§: <50% gÃ¢y suy giáº£m máº¡nh embedding alignment; >50% lá»£i Ã­ch nÃ©n khÃ´ng rÃµ rá»‡t. Ablation ná»™i bá»™ cho tháº¥y 0.5 tá»‘i Æ°u.

---
## 4. Loss Functions
### Q8: CÃ¡c thÃ nh pháº§n loss gá»“m gÃ¬?
- Ngáº¯n: student_contrast + KD + augmentation_alignment + (optional) link_reconstruction.
- Technical pseudo:
```python
Total = student_loss \
      + main_distill_weight * kd_loss \
      + augmentation_weight * augmentation_alignment_loss \
      + link_recon_weight * link_recon_loss
```
### Q9: KD loss cá»¥ thá»ƒ lÃ  gÃ¬?
- Technical: L2/MSE trÃªn embeddings Ä‘Ã£ chuáº©n hoÃ¡ + alignment head (MLP + LayerNorm) giá»¯a student vÃ  teacher meta-path & schema representations.

### Q10: Augmentation alignment lÃ  gÃ¬?
- Ngáº¯n: Student Ä‘iá»u chá»‰nh hÆ°á»›ng biá»ƒu diá»…n theo trá»ng sá»‘ importance tá»« augmentation teacher.
- Technical: So sÃ¡nh hoáº·c Ä‘iá»u hÃ²a phÃ¢n phá»‘i attention / guidance vectors vá»›i embedding student, thÆ°á»ng qua MSE hoáº·c cosine + weighting.

### Q11: Link reconstruction loss Ä‘á»ƒ lÃ m gÃ¬?
- Tráº£ lá»i: Bá»• sung tÃ­n hiá»‡u cáº¥u trÃºc rÃµ rÃ ng giÃºp student khÃ´ng chá»‰ dá»±a vÃ o tÆ°Æ¡ng quan embedding mÃ  cÃ²n mÃ´ hÃ¬nh hÃ³a xÃ¡c suáº¥t cáº¡nh; tÄƒng kháº£ nÄƒng link prediction.

---
## 5. Robustness & Generalization
### Q12: Robustness Ä‘áº¡t Ä‘Æ°á»£c báº±ng cÃ¡ch nÃ o?
- Ngáº¯n: Há»c tá»« augmented meta-path graph + alignment guidance.
- Technical: Augmentation pipeline sá»­ dá»¥ng structure-aware meta-path propagation (controlled multi-hop connections), low-rank projection (parameter regularization), semantic-level meta-path attention (importance weighting), vÃ  residual mixing (GCNII-style, alpha=0.15). Embeddings augmented â†’ guidance vectors (mp_guidance, sc_guidance, attention_importance, structure_importance) â†’ alignment vÃ o student. KHÃ”NG sá»­ dá»¥ng feature masking hay edge perturbation.

### Q13: Lá»£i Ã­ch cá»¥ thá»ƒ?
- Liá»‡t kÃª: giáº£m overfitting, giá»¯ ~95% accuracy sau nÃ©n, á»•n Ä‘á»‹nh trÆ°á»›c thay Ä‘á»•i nhá», cáº£i thiá»‡n link prediction consistency.

---
## 6. Evaluation & Metrics
### Q14: ÄÃ¡nh giÃ¡ gÃ¬?
- Node classification (Accuracy, Macro-F1, Micro-F1), Link prediction (AUC/AP), Clustering (NMI/ARI), Compression (params, memory, latency).

### Q15: Chá»©ng cá»© Ä‘á»‹nh lÆ°á»£ng?
- VÃ­ dá»¥: Teacher vs Student: ~50% params â†’ ~95% retention accuracy (ACM), thá»i gian suy luáº­n giáº£m ~50%.

---
## 7. So sÃ¡nh & Ablation
### Q16: Náº¿u chá»‰ dÃ¹ng teacher chÃ­nh?
- Máº¥t robustness; retention tÆ°Æ¡ng tá»± ban Ä‘áº§u nhÆ°ng suy giáº£m khi augmentations test-time.
### Q17: Náº¿u tÄƒng augmentation_weight quÃ¡ cao?
- Nguy cÆ¡ lÃ m lá»‡ch semantic space â†’ giáº£m Ä‘á»™ chÃ­nh xÃ¡c phÃ¢n loáº¡i.
### Q18: CÃ³ thá»ƒ há»c trá»ng sá»‘ Î±, Î² Ä‘á»™ng?
- CÃ³: DÃ¹ng má»™t gating network hoáº·c uncertainty weighting.

---
## 8. Má»Ÿ Rá»™ng & TÆ°Æ¡ng Lai
### Q19: HÆ°á»›ng phÃ¡t triá»ƒn tiáº¿p?
- Dynamic weighting, multi-level contrast, attention distillation, meta-path curriculum.
### Q20: á»¨ng dá»¥ng thá»±c táº¿?
- Há»‡ thá»‘ng khuyáº¿n nghá»‹ Ä‘a thá»±c thá»ƒ, tri thá»©c y sinh, máº¡ng trÃ­ch xuáº¥t quan há»‡ há»c thuáº­t.

---
## 9. Dáº¡ng Tráº£ Lá»i Tiáº¿ng Anh (Sample)
Q: What is the role of the augmentation teacher?
A: It operates as a robustness expert trained on perturbed heterogeneous graphs, producing structural importance signals that guide the compressed student toward perturbation-stable representation subspaces.

Q: Why dual-teacher instead of standard KD?
A: Standard KD transfers semantic knowledge only; dual-teacher adds a robustness dimension, reducing overfitting and improving stability under structural variations.

---
## 10. Máº¹o TrÃ¬nh BÃ y Nhanh
- Nháº¥n máº¡nh: Hai miá»n tri thá»©c (clean vs robust).
- Sá»‘ liá»‡u: 50% params, ~95% retention.
- LÃ½ do: Trade-off precision vs resilience.
- KhÃ¡c biá»‡t: KhÃ´ng nÃ©n middle teacher -> tÃ­n hiá»‡u augmentation phong phÃº.

---
## 11. Template CÃ¢u Tráº£ Lá»i Nhanh
"Middle teacher cá»§a em lÃ  má»™t augmentation expert há»c Ä‘á»™c láº­p trÃªn Ä‘á»“ thá»‹ biáº¿n Ä‘á»•i. NÃ³ táº¡o ra cÃ¡c trá»ng sá»‘ meta-path/schema vÃ  cáº¥u trÃºc á»•n Ä‘á»‹nh. Student dÃ¹ng cáº£ KD tá»« teacher chÃ­nh vÃ  alignment tá»« expert Ä‘á»ƒ vá»«a giá»¯ Ä‘á»™ chÃ­nh xÃ¡c vá»«a chá»‘ng nhiá»…u vá»›i chá»‰ 50% tham sá»‘." 

---
## 12. Ghi ChÃº
CÃ³ thá»ƒ bá»• sung thÃªm vÃ­ dá»¥ cá»¥ thá»ƒ tá»« log huáº¥n luyá»‡n náº¿u cáº§n.

---
## 13. Äá» Xuáº¥t Cáº­p Nháº­t
- ThÃªm script kiá»ƒm tra robustness.
- ThÃªm benchmark 'without augmentation teacher'.
- Ghi láº¡i thá»i gian suy luáº­n so sÃ¡nh.

---
## 14. Augmentation Teacher cÃ³ â€œgiÃ u ngá»¯ nghÄ©aâ€ hÆ¡n khÃ´ng? (Q21)
### CÃ¢u há»i
"Táº¡i sao teacher chÃ­nh há»c trÃªn dá»¯ liá»‡u sáº¡ch cÃ²n augmentation teacher há»c trÃªn dá»¯ liá»‡u biáº¿n Ä‘á»•i â€“ augmentation teacher cÃ³ pháº£i giÃ u ngá»¯ nghÄ©a hÆ¡n khÃ´ng?"

### Tráº£ lá»i Ä‘a cáº¥p Ä‘á»™
- One-liner: Augmentation teacher khÃ´ng â€˜sáº¡ch hÆ¡nâ€™ mÃ  â€˜Ä‘a dáº¡ng hÃ³aâ€™ cÃ¡c quan há»‡ giÃ¡n tiáº¿p Ä‘á»ƒ tÄƒng robustness.
- Ngáº¯n gá»n: NÃ³ khuáº¿ch Ä‘áº¡i Ä‘Æ°á»ng Ä‘i meta-path vÃ  máº«u cáº¥u trÃºc thay Ä‘á»•i cÃ³ kiá»ƒm soÃ¡t; tri thá»©c semantic gá»‘c váº«n Ä‘áº¿n tá»« teacher chÃ­nh, augmentation cung cáº¥p invariance + trá»ng sá»‘ chÃº Ã½.
- Ká»¹ thuáº­t:
  1. Clean teacher tá»‘i Æ°u biá»ƒu diá»…n ná»n táº£ng (meta-path + schema) trÃªn phÃ¢n phá»‘i gá»‘c.
  2. Augmentation teacher sá»­ dá»¥ng meta-path attention + low-rank projections + residual Ä‘á»ƒ táº¡o embedding chá»‹u Ä‘Æ°á»£c biáº¿n thiÃªn.
  3. TÃ­n hiá»‡u â€˜giÃ uâ€™ = thÃªm quan há»‡ giÃ¡n tiáº¿p (propagated meta-path), khÃ´ng pháº£i fidelity cao hÆ¡n; nguy cÆ¡ over-smoothing náº¿u xem Ä‘Ã³ lÃ  semantics thuáº§n.
  4. Student cÃ¢n báº±ng báº±ng trá»ng sá»‘ (main_distill_weight, augmentation_weight) Ä‘á»ƒ trÃ¡nh drift.
- Há»c thuáº­t: The robustness expert enriches connectivity patterns through controlled meta-path propagation and attention, but semantic fidelity remains anchored by the primary teacher to prevent representational drift.
- Phá»ng váº¥n: â€œChÃºng tÃ´i tÃ¡ch hai miá»n: teacher chÃ­nh giá»¯ tri thá»©c sáº¡ch; augmentation teacher nháº¥n máº¡nh invariance. Káº¿t há»£p cáº£ hai trÃ¡nh overfit vÃ  váº«n á»•n Ä‘á»‹nh dÆ°á»›i perturbation.â€

### Sai láº§m phá»• biáº¿n
| Hiá»ƒu nháº§m | Há»‡ quáº£ | Äiá»u chá»‰nh |
|-----------|--------|------------|
| â€˜Augmented = semantic hÆ¡nâ€™ | Student lá»‡ch khá»i phÃ¢n phá»‘i gá»‘c | Giá»¯ KD weight Ä‘á»§ lá»›n |
| TÄƒng augmentation_weight quÃ¡ má»©c | Giáº£m accuracy | Grid search Î±, Î² |
| Bá» clean teacher | Máº¥t anchor, dá»… drift | LuÃ´n giá»¯ teacher chÃ­nh |

### Khuyáº¿n nghá»‹ thá»±c nghiá»‡m
```python
# Pseudo kiá»ƒm Ä‘á»‹nh: Ä‘o similarity vÃ  robustness
with torch.no_grad():
    t_mp, t_sc = teacher.get_representations(feats, mps, nei_index)
    a_mp, a_sc = aug_teacher.get_representations(feats, mps, nei_index)

sim_meta = F.cosine_similarity(t_mp, a_mp).mean().item()
sim_schema = F.cosine_similarity(t_sc, a_sc).mean().item()
print('Meta-path similarity:', sim_meta)
print('Schema similarity:', sim_schema)
```

### Khi nÃ o augmentation giÃºp
- Graph cÃ³ cáº¥u trÃºc thay Ä‘á»•i theo thá»i gian.
- Nhiá»u meta-path dÃ i gÃ¢y nhiá»…u náº¿u khÃ´ng chuáº©n hÃ³a.
- Má»¥c tiÃªu triá»ƒn khai model nÃ©n trong mÃ´i trÆ°á»ng Ä‘á»™ng.

### Khi nÃ o nÃªn giáº£m vai trÃ² augmentation
- Dataset nhá», Ã­t nhiá»…u.
- Meta-path Ä‘Æ¡n giáº£n, Ã­t Ä‘a dáº¡ng.
- Æ¯u tiÃªn latency cá»±c nhanh hÆ¡n robustness.

---
## 15. Hyperparameter Cheat Sheet (Q22)
### Q22: CÃ¡c hyperparameter quan trá»ng vÃ  khuyáº¿n nghá»‹ ban Ä‘áº§u?
| TÃªn | Vai trÃ² | GiÃ¡ trá»‹ gá»£i Ã½ | Khi tÄƒng | Khi giáº£m |
|-----|---------|--------------|----------|----------|
| `compression_ratio` | Tá»· lá»‡ nÃ©n student | 0.5 | Giáº£m chi phÃ­, rá»§i ro máº¥t fidelity náº¿u <0.4 | TÄƒng tham sá»‘, lá»£i Ã­ch nÃ©n giáº£m náº¿u >0.6 |
| `main_distill_weight` | Trá»ng sá»‘ KD semantic | 1.0 | Neo semantic máº¡nh hÆ¡n, chá»‘ng drift | Student dá»… bá»‹ lá»‡ch theo robustness |
| `augmentation_weight` | Trá»ng sá»‘ alignment robustness | 0.5 | TÄƒng invariance, nguy cÆ¡ máº¥t precision náº¿u >0.8 | Giáº£m robustness |
| `link_recon_weight` | Bá»• sung cáº¥u trÃºc rÃµ rÃ ng | 0.1â€“0.3 | Cáº£i thiá»‡n link prediction | Ãt rÃ ng buá»™c cáº¥u trÃºc |
| `alpha` (residual aug) | Trá»™n clean & aug embeddings | 0.15 | Giá»¯ clean anchor | Aug embedding chi phá»‘i quÃ¡ máº¡nh náº¿u quÃ¡ cao |
| `lr` | Learning rate | 1e-3 AdamW | Há»c nhanh hÆ¡n, nguy cÆ¡ khÃ´ng á»•n Ä‘á»‹nh | Há»c cháº­m |
| `warmup_epochs` | á»”n Ä‘á»‹nh Ä‘áº§u huáº¥n luyá»‡n | 3â€“5 | Giáº£m biáº¿n Ä‘á»™ng gradient | CÃ³ thá»ƒ máº¥t thá»i gian náº¿u quÃ¡ dÃ i |

Quick heuristic: Giá»¯ (main_distill_weight >= augmentation_weight) trong giai Ä‘oáº¡n Ä‘áº§u; tinh chá»‰nh augmentation_weight sau khi student Ä‘Ã£ tÃ¡i táº¡o semantic.

## 16. Robustness Evaluation Script (Q23)
### Q23: Pseudo-code kiá»ƒm tra robustness nhÆ° tháº¿ nÃ o?
```python
def evaluate_robustness(model, data, perturbations=(0.0, 0.05, 0.1, 0.2)):
    base_edges = data.edge_index.clone()
    results = {}
    for p in perturbations:
        if p == 0.0:
            edge_index = base_edges
        else:
            edge_index = random_edge_dropout(base_edges, drop_prob=p)
        out = model(data.x, edge_index)
        acc = compute_accuracy(out[data.test_mask], data.y[data.test_mask])
        results[p] = acc
    return results

student = load_student_checkpoint(path)
teacher = load_teacher_checkpoint(path_teacher)
rob_student = evaluate_robustness(student, dataset)
rob_teacher = evaluate_robustness(teacher, dataset)
print('Robustness student:', rob_student)
print('Robustness teacher:', rob_teacher)
```
Metrics: Giá»¯ suy giáº£m <5% khi p <= 0.1 lÃ  tá»‘t; augmentation teacher nÃªn giÃºp Ä‘Æ°á»ng cong suy giáº£m mÆ°á»£t hÆ¡n.

## 17. Ablation Recipes (Q24)
### Q24: Thiáº¿t káº¿ ablation tá»‘i thiá»ƒu?
1. Teacher chÃ­nh (baseline).
2. Teacher chÃ­nh + Augmentation teacher (khÃ´ng nÃ©n student, chá»‰ Ä‘Ã¡nh giÃ¡ robustness gain).
3. Student + main KD (khÃ´ng augmentation).
4. Student + dual-teacher KD (Ä‘áº§y Ä‘á»§).
5. Student + dual-teacher KD + link reconstruction.

Report báº£ng: Params | Accuracy | Î” vs teacher | Robustness@10% noise | Inference ms. Highlight retention vÃ  Ä‘á»™ dá»‘c suy giáº£m.

## 18. Tuning & Failure Modes (Q25)
### Q25: Dáº¥u hiá»‡u sai vÃ  cÃ¡ch chá»‰nh?
| Triá»‡u chá»©ng | NguyÃªn nhÃ¢n kháº£ dÄ© | CÃ¡ch kháº¯c phá»¥c |
|-------------|--------------------|----------------|
| Accuracy giáº£m máº¡nh khi thÃªm augmentation | augmentation_weight quÃ¡ cao | Giáº£m augmentation_weight hoáº·c tÄƒng main_distill_weight |
| Embedding norm student bÃ¹ng ná»• | LR cao, thiáº¿u LayerNorm | Giáº£m lr, thÃªm norm projection |
| Robustness khÃ´ng cáº£i thiá»‡n | connection_strength quÃ¡ tháº¥p hoáº·c alpha quÃ¡ cao | TÄƒng connection_strength (0.1â†’0.2) hoáº·c giáº£m alpha residual |
| Link prediction kÃ©m | link_recon_weight quÃ¡ tháº¥p hoáº·c táº¯t | Báº­t lÃªn 0.1â€“0.2 |
| Máº¥t semantic fidelity | KD weight tháº¥p, projection kÃ©m | TÄƒng main_distill_weight, kiá»ƒm tra alignment MLP |

Checklist tuning tuáº§n tá»±:
1. Äáº£m báº£o student há»c semantic: freeze augmentation (augmentation_weight=0) vÃ i epoch Ä‘áº§u náº¿u cáº§n.
2. Má»Ÿ augmentation_weight dáº§n: 0.3 â†’ 0.5.
3. Báº­t link_recon náº¿u task yÃªu cáº§u link prediction.
4. Äo robustness curve (0%,5%,10%,20%).
5. Äiá»u chá»‰nh Ä‘á»ƒ Î”Accuracy@0% <5% vÃ  Î”Robustness@10% <2% so vá»›i teacher chÃ­nh.

Optional advanced: DÃ¹ng uncertainty weighting: w_i = 1 / (2 * sigma_i^2) vá»›i sigma_i cáº­p nháº­t Ä‘á»™ng theo moving average loss.

---
## 19. Táº¡i Sao KhÃ´ng DÃ¹ng Feature Masking / Edge Perturbation? (Q26)
### Q26: Augmentation khÃ´ng dÃ¹ng feature masking hay edge dropout - táº¡i sao?

#### Tráº£ lá»i Ä‘a cáº¥p Ä‘á»™
- One-liner: ChÃºng tÃ´i Æ°u tiÃªn structure-aware meta-path propagation Ä‘á»ƒ báº£o toÃ n semantic integrity thay vÃ¬ stochastic perturbations gÃ¢y máº¥t thÃ´ng tin.
- Pitch: Feature masking vÃ  edge dropout lÃ  ká»¹ thuáº­t há»¯u Ã­ch cho homogeneous graphs, nhÆ°ng vá»›i heterogeneous graphs cÃ³ meta-paths phá»©c táº¡p, viá»‡c ngáº«u nhiÃªn xÃ³a features hoáº·c cáº¡nh cÃ³ thá»ƒ phÃ¡ vá»¡ quan há»‡ semantic quan trá»ng giá»¯a cÃ¡c node types. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i dÃ¹ng controlled meta-path propagation + low-rank projection Ä‘á»ƒ táº¡o diversity mÃ  váº«n giá»¯ cáº¥u trÃºc ngá»¯ nghÄ©a.

#### Technical Explanation
**Táº¡i sao KHÃ”NG dÃ¹ng feature masking:**
1. **Semantic Preservation**: Trong heterogeneous graphs (ACM: paper-author-subject), features cá»§a má»—i node type cÃ³ Ã½ nghÄ©a khÃ¡c nhau. Random masking cÃ³ thá»ƒ xÃ³a máº¥t thÃ´ng tin quan trá»ng (vÃ­ dá»¥: keyword chÃ­nh cá»§a paper).
2. **Heterogeneity Complexity**: KhÃ´ng rÃµ nÃªn mask bao nhiÃªu % cho tá»«ng node type; mask quÃ¡ nhiá»u â†’ máº¥t semantic, mask quÃ¡ Ã­t â†’ khÃ´ng Ä‘á»§ augmentation.
3. **Alternative Regularization**: Low-rank projection (dim â†’ k â†’ dim) Ä‘Ã£ cung cáº¥p regularization tÆ°Æ¡ng tá»± nhÆ°ng cÃ³ kiá»ƒm soÃ¡t hÆ¡n.

**Táº¡i sao KHÃ”NG dÃ¹ng edge perturbation:**
1. **Meta-path Integrity**: Meta-paths (PAP, PSP) Ä‘Æ°á»£c tÃ­nh toÃ¡n tá»« cáº¥u trÃºc graph gá»‘c. Random dropout edges sáº½ phÃ¡ vá»¡ cÃ¡c Ä‘Æ°á»ng Ä‘i meta-path, lÃ m máº¥t tÃ­nh nháº¥t quÃ¡n.
2. **Structural Semantics**: Má»—i cáº¡nh trong heterogeneous graph mang thÃ´ng tin quan há»‡ typed (paper-author, paper-subject). XÃ³a ngáº«u nhiÃªn gÃ¢y máº¥t cÃ¢n báº±ng type distribution.
3. **Propagation-based Diversity**: Meta-path propagation tá»± nhiÃªn táº¡o "soft perturbation" báº±ng cÃ¡ch khuáº¿ch Ä‘áº¡i multi-hop connections, khÃ´ng cáº§n dropout cá»©ng.

**Thay vÃ o Ä‘Ã³, chÃºng tÃ´i dÃ¹ng:**
- **Meta-path Propagation**: Expand neighborhood qua controlled multi-hop connections â†’ tÄƒng receptive field mÃ  khÃ´ng máº¥t cáº¥u trÃºc.
- **Low-rank Projection**: Bottleneck (dim â†’ 64 â†’ dim) â†’ regularization + giáº£m parameters.
- **Semantic Attention**: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh trá»ng sá»‘ meta-paths theo importance â†’ adaptive augmentation.
- **Residual Mixing** (alpha=0.15): Giá»¯ clean anchor `(1+Î±)*feat + (1-Î±)*aug_signal` â†’ chá»‘ng over-smoothing.

#### So sÃ¡nh Augmentation Strategies
| Chiáº¿n lÆ°á»£c | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | PhÃ¹ há»£p cho |
|------------|---------|------------|-------------|
| **Feature Masking** | ÄÆ¡n giáº£n, hiá»‡u quáº£ cho CV/NLP | Máº¥t thÃ´ng tin, khÃ³ tune cho heterogeneous | Homogeneous graphs, rich features |
| **Edge Dropout** | TÄƒng robustness to missing edges | PhÃ¡ vá»¡ meta-path structure | Homogeneous graphs, simple topology |
| **Meta-path Propagation** (ours) | Giá»¯ semantic structure, controlled diversity | Cáº§n meta-path adjacency matrices | Heterogeneous graphs, typed relations |

#### Khi nÃ o nÃªn cÃ¢n nháº¯c thÃªm stochastic augmentation
- **Homogeneous graph** vá»›i cáº¥u trÃºc Ä‘Æ¡n giáº£n â†’ cÃ³ thá»ƒ thÃªm edge dropout nháº¹ (5-10%).
- **Feature space ráº¥t lá»›n** (>10K dims) â†’ cÃ³ thá»ƒ thÃªm feature dropout nháº¹ (10-20%).
- **Ablation experiment** muá»‘n so sÃ¡nh vá»›i baseline augmentations.

#### Pseudo-code minh há»a sá»± khÃ¡c biá»‡t
```python
# âŒ KHÃ”NG dÃ¹ng (stochastic perturbation):
def stochastic_augmentation(feats, edges):
    masked_feats = feats * bernoulli_mask(p=0.2)  # Random mask 20%
    perturbed_edges = edge_dropout(edges, p=0.1)  # Random drop 10%
    return masked_feats, perturbed_edges

# âœ… DÃ™NG (structure-aware propagation):
def structure_aware_augmentation(feats, meta_path_matrices):
    projected = low_rank_projection(feats)  # dim â†’ 64 â†’ dim
    propagated = meta_path_attention_propagation(projected, meta_path_matrices)
    aug_signal = connection_strength * gating(propagated)
    return (1 + alpha) * feats + (1 - alpha) * aug_signal  # Residual mix
```

#### Academic Justification
While stochastic feature masking and edge perturbation are effective for homogeneous graphs (where nodes/edges are type-uniform), heterogeneous graphs require preserving typed structural semantics. Our structure-aware meta-path propagation approach provides controlled augmentation that respects heterogeneity: it amplifies multi-hop relational signals without destroying the semantic integrity of typed connections, achieving robustness through structural expansion rather than information removal.

#### Metrics chá»©ng minh hiá»‡u quáº£
- **Semantic Retention**: Cosine similarity giá»¯a clean vÃ  augmented embeddings > 0.85 (cao hÆ¡n masking/dropout ~0.6-0.7).
- **Robustness Gain**: Î”Accuracy khi test-time edge dropout 10% giáº£m <3% (masking/dropout ~5-7%).
- **Parameter Efficiency**: Low-rank giáº£m 55x params so vá»›i full projection.

---
## 20. PhÆ°Æ¡ng PhÃ¡p Augmentation Chi Tiáº¿t (Q27)
### Q27: PhÆ°Æ¡ng phÃ¡p augmented graph cá»§a báº¡n nhÆ° tháº¿ nÃ o? CÃ³ Ã½ nghÄ©a vÃ  vai trÃ² gÃ¬? Hoáº¡t Ä‘á»™ng ra sao?

#### One-liner
ChÃºng tÃ´i dÃ¹ng structure-aware meta-path propagation vá»›i low-rank projection vÃ  semantic attention Ä‘á»ƒ táº¡o augmented embeddings bá»n vá»¯ng hÆ¡n, khÃ´ng phÃ¡ vá»¡ cáº¥u trÃºc heterogeneous graph.

#### Pitch (30â€“45 giÃ¢y)
Thay vÃ¬ random masking features hoáº·c dropout edges (dá»… phÃ¡ vá»¡ ngá»¯ nghÄ©a trong heterogeneous graph), chÃºng tÃ´i khuáº¿ch Ä‘áº¡i thÃ´ng tin cáº¥u trÃºc cÃ³ sáºµn qua **meta-path propagation cÃ³ kiá»ƒm soÃ¡t**. Cá»¥ thá»ƒ:
1. Project features qua low-rank bottleneck (dim â†’ 64 â†’ dim) Ä‘á»ƒ regularize
2. Propagate qua meta-path adjacency matrices (PAP, PSP) vá»›i semantic attention
3. Trá»™n vá»›i features gá»‘c qua residual connection (alpha=0.15)

Káº¿t quáº£: embeddings cÃ³ thÃªm multi-hop context mÃ  khÃ´ng máº¥t thÃ´ng tin gá»‘c, giÃºp model há»c Ä‘Æ°á»£c biá»ƒu diá»…n robust trÆ°á»›c structural variations.

#### Technical: Kiáº¿n TrÃºc Pipeline

```
Input: feats (node features), mps (meta-path adjacency matrices)
  â†“
[1] Low-Rank Projection: dim â†’ 64 â†’ dim (giáº£m 55x params)
  â†“
[2] Meta-Path Propagation:
    - Single path: torch.sparse.mm(mp_matrix, projected_feat)
    - Multiple paths: Semantic Attention (HAN-style)
      * Compute attention: softmax(attention_net(mp_repr))
      * Weighted sum: Î£(attn_weight_i * propagated_i)
  â†“
[3] Gating: connection_strength * sigmoid(learnable_emb) * propagated
  â†“
[4] Residual Mixing: (1 + Î±) * feat + (1 - Î±) * aug_signal (Î±=0.15)
  â†“
Output: augmented_feats (same shape as input)
```

#### CÃ¡c ThÃ nh Pháº§n Chi Tiáº¿t

**1. Low-Rank Projection**
```python
# Code implementation:
nn.Sequential(
    nn.Linear(dim, 64, bias=False),  # Bottleneck
    nn.Linear(64, dim, bias=False)   # Expand
)
# ACM example: 7167Â² = 51M â†’ 2*7167*64 = 917K (giáº£m 55x)
```
- **Vai trÃ²**: Regularization, giáº£m overfitting, parameter efficiency
- **Ã nghÄ©a**: Buá»™c model há»c compressed representation trÆ°á»›c propagation

**2. Meta-Path Propagation**
```python
# Single meta-path:
propagated = torch.sparse.mm(meta_path_matrix, projected_features)

# Multiple meta-paths vá»›i attention:
attn_weights = softmax([attention_net(mp_i) for mp_i in metapaths])
propagated = Î£(attn_weights[i] * propagated_i)
```
- **Vai trÃ²**: Khuáº¿ch Ä‘áº¡i multi-hop semantic connections
- **Ã nghÄ©a**: Node Ã­t káº¿t ná»‘i Ä‘Æ°á»£c enriched bá»Ÿi neighbors qua meta-paths

**3. Semantic-Level Attention** (khi >1 meta-path)
```python
# Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh importance tá»«ng meta-path
mp_reprs = stacked_outputs.mean(dim=1)  # Graph-level pooling
attn_logits = [attention_net(mp_repr) for mp_repr in mp_reprs]
attn_weights = F.softmax(attn_logits, dim=0)
```
- **Vai trÃ²**: Adaptive weighting cho tá»«ng meta-path
- **Ã nghÄ©a**: PAP vÃ  PSP cÃ³ contribution khÃ¡c nhau â†’ tá»± Ä‘iá»u chá»‰nh

**4. Residual Mixing** (GCNII-inspired)
```python
alpha = 0.15  # Hyperparameter
connected_feat = (1 + alpha) * original_feat + (1 - alpha) * aug_signal
```
- **Vai trÃ²**: Chá»‘ng over-smoothing, giá»¯ semantic anchor
- **Ã nghÄ©a**: Balance giá»¯a original information vÃ  augmented context

#### Vai TrÃ² Trong Dual-Teacher KD

| Vai trÃ² | CÃ¡ch thá»±c hiá»‡n | Káº¿t quáº£ |
|---------|----------------|---------|
| **Táº¡o Diversity** | Propagate qua meta-paths táº¡o alternative view | Aug teacher há»c tá»« view khÃ¡c vs clean |
| **Robustness Signal** | Embeddings á»•n Ä‘á»‹nh dÆ°á»›i variations | Student há»c invariance qua alignment |
| **Guidance Generation** | Teacher tÃ­nh importance tá»« aug embeddings | Student nháº­n weighted guidance |
| **Regularization** | Low-rank bottleneck + controlled propagation | TrÃ¡nh overfitting cáº¥u trÃºc gá»‘c |

#### Hoáº¡t Äá»™ng Cá»¥ Thá»ƒ (Forward Pass)

```python
# 1. Augment features
aug_feats, aug_info = augmentation_pipeline(feats, mps)

# 2. Augmentation teacher xá»­ lÃ½ cáº£ clean & augmented
z_mp_orig = mp_encoder(feats[0], mps)
z_mp_aug = mp_encoder(aug_feats[0], mps)  # Tá»« augmented features

# 3. Generate guidance tá»« augmented embeddings
mp_guidance = mp_guide_network(z_mp_aug)  # [1, P]
sc_guidance = sc_guide_network(z_sc_aug)  # [1, nei_num]
attention_importance = attention_predictor(z_mp_aug, z_sc_aug)  # [batch, 1]
structure_importance = structure_predictor(z_mp_aug)  # [batch, expert_dim]

# 4. Student alignment vá»›i guidance
alignment_loss = MSE(student_attention, attention_importance)
```

#### Academic Explanation

**Proposed Augmentation Strategy:**

We introduce a structure-aware meta-path propagation mechanism for heterogeneous graphs, avoiding stochastic feature masking or edge perturbation that may compromise typed semantic relationships. Our approach consists of:

1. **Low-Rank Projection**: $\mathbf{h}' = W_2 \cdot \text{ReLU}(W_1 \cdot \mathbf{h})$ where $W_1 \in \mathbb{R}^{d \times k}, W_2 \in \mathbb{R}^{k \times d}, k=64 \ll d$. Reduces parameters by 98% while preserving expressive power.

2. **Semantic Meta-Path Propagation**: For meta-path $\mathcal{M}_i$ with adjacency $A_i$:
   $$\mathbf{H}_{\text{aug}} = \sum_{i=1}^{P} \alpha_i A_i \mathbf{H}', \quad \alpha_i = \frac{\exp(\mathbf{w}^\top \tanh(\bar{\mathbf{H}}_i'))}{\sum_j \exp(\mathbf{w}^\top \tanh(\bar{\mathbf{H}}_j'))}$$

3. **Controlled Gating**: $\mathbf{S} = \beta \cdot (\sigma(\mathbf{e}) \odot \mathbf{H}_{\text{aug}})$ vá»›i $\beta=0.1$ (connection strength), $\mathbf{e}$ learnable.

4. **Residual Anchoring**: $(1 + \gamma) \mathbf{H} + (1 - \gamma) \mathbf{S}$, $\gamma=0.15$ prevents over-smoothing.

**Rationale:** Unlike homogeneous graph augmentations relying on information removal (dropout/masking), our method enriches representations through controlled structural expansion, preserving heterogeneous semantics while inducing topological robustness.

#### So SÃ¡nh Vá»›i CÃ¡c PhÆ°Æ¡ng PhÃ¡p KhÃ¡c

| Method | Augmentation Strategy | Pros | Cons | Heterogeneous? |
|--------|----------------------|------|------|----------------|
| **GraphCL** | Node/edge dropout, attribute masking | Simple, effective | Breaks typed relations | âŒ |
| **BGRL** | Corruption + bootstrapping | Self-supervised | Random corruption loses semantics | âŒ |
| **MVGRL** | Diffusion + random walk | Multi-view learning | Expensive, not structure-aware | âš ï¸ |
| **Ours** | Structure-aware meta-path propagation | Preserves heterogeneity | Needs meta-path matrices | âœ… |

#### Metrics Chá»©ng Minh Hiá»‡u Quáº£

```python
# 1. Diversity: Embeddings khÃ¡c nhau nhÆ°ng khÃ´ng quÃ¡ xa
diversity = 1 - F.cosine_similarity(clean_emb, aug_emb).mean()
# Expected: 0.1 - 0.3

# 2. Robustness: Test vá»›i perturbed graph
acc_clean = evaluate(model, edge_dropout=0.0)
acc_perturbed = evaluate(model, edge_dropout=0.1)
robustness_degradation = (acc_clean - acc_perturbed) / acc_clean
# Expected: <5%

# 3. Semantic retention (trÃ¡nh over-smoothing)
variance_ratio = aug_emb.var(dim=0).mean() / clean_emb.var(dim=0).mean()
# Expected: >0.8
```

#### Æ¯u Äiá»ƒm & Háº¡n Cháº¿

**Æ¯u Ä‘iá»ƒm:**
- **Semantic Preservation**: KhÃ´ng phÃ¡ typed relationships
- **Parameter Efficiency**: Low-rank giáº£m 55x params
- **Adaptive Importance**: Semantic attention tá»± Ä‘iá»u chá»‰nh
- **Controlled Diversity**: Residual mixing trÃ¡nh drift
- **End-to-End Differentiable**: KhÃ´ng cáº§n sampling

**Háº¡n cháº¿:**
- Cáº§n precompute meta-path matrices (PAP, PSP)
- LÆ°u cáº£ clean + augmented embeddings (2x memory)
- Hyperparameters: Î± (residual), Î² (connection_strength) cáº§n tune

#### Template Tráº£ Lá»i Nhanh (Defense/Interview)

> "PhÆ°Æ¡ng phÃ¡p augmentation cá»§a chÃºng tÃ´i **khÃ´ng dÃ¹ng random masking hay edge dropout** vÃ¬ chÃºng dá»… phÃ¡ vá»¡ cáº¥u trÃºc semantic trong heterogeneous graph. Thay vÃ o Ä‘Ã³, chÃºng tÃ´i **khuáº¿ch Ä‘áº¡i thÃ´ng tin multi-hop** qua meta-path propagation Ä‘Æ°á»£c kiá»ƒm soÃ¡t bá»Ÿi semantic attention vÃ  low-rank projection. Vá»›i ACM dataset cÃ³ PAP vÃ  PSP, chÃºng tÃ´i propagate features qua cáº£ hai ma tráº­n, dÃ¹ng attention tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh trá»ng sá»‘ tá»«ng path, rá»“i trá»™n vá»›i features gá»‘c qua residual (alpha=0.15) Ä‘á»ƒ trÃ¡nh over-smoothing. Káº¿t quáº£: augmentation teacher há»c Ä‘Æ°á»£c embeddings robust hÆ¡n ~3-5% khi test dÆ°á»›i perturbation, Ä‘á»“ng thá»i giá»¯ ~90% cosine similarity vá»›i clean embeddings."

---
**End of Question Bank**
